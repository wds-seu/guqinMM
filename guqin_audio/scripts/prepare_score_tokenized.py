import torch
import os
import argparse 

def prepare_score_tokenized(score_tokenized_path, output_path, n_pieces=5):
    """
    - score_tokenized_path: åŸå§‹2651ä¸ªå°ç‰‡æ®µç»„æˆçš„list
    - output_path: ä¿å­˜çš„æ–°ç‰ˆæœ¬.pt
    - n_pieces: è¦åˆ‡æˆå‡ é¦–æ›²å­ï¼Œé»˜è®¤5
    """

    score_tokenized = torch.load(score_tokenized_path)  # åŠ è½½2651æ¡tokenç‰‡æ®µ

    if not isinstance(score_tokenized, list):
        raise ValueError("Input score_tokenized should be a list.")

    total = len(score_tokenized)
    per_piece = total // n_pieces  # æ¯é¦–æ›²å­å¤§çº¦å¤šå°‘æ¡
    samples = {}

    print(f"ğŸ” Total {total} segments, splitting into {n_pieces} pieces, about {per_piece} per piece...")

    for i in range(n_pieces):
        start_idx = i * per_piece
        end_idx = (i + 1) * per_piece if i != n_pieces - 1 else total  # æœ€åä¸€é¦–æ‹¿å‰©ä¸‹æ‰€æœ‰

        merged_tokens = []
        for j in range(start_idx, end_idx):
            line_tokens = score_tokenized[j]
            merged_tokens.extend(line_tokens)  # æŠŠè¿™ä¸€è¡Œçš„æ‰€æœ‰tokenåŠ å…¥

        piece_name = f"piece_{i+1}"  # ç»Ÿä¸€å‘½åï¼špiece_1, piece_2, ...

        samples[piece_name] = {
            "tokens": merged_tokens,
            "style": 3  # é»˜è®¤æµæ´¾æ ‡ç­¾ other=3
        }

        print(f"âœ… {piece_name}: {len(merged_tokens)} tokens.")

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    torch.save(samples, output_path)
    print(f"âœ… Saved new score_tokenized to {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--input_path', type=str, required=True, help="åŸå§‹å°ç‰‡æ®µè°±æ–‡tokensè·¯å¾„")
    parser.add_argument('--output_path', type=str, required=True, help="è¾“å‡ºçš„åˆå¹¶åè°±æ–‡tokensè·¯å¾„")
    parser.add_argument('--n_pieces', type=int, default=5, help="è¦åˆ‡æˆå¤šå°‘é¦–æ›²å­")
    args = parser.parse_args()

    prepare_score_tokenized(
        score_tokenized_path=args.input_path,
        output_path=args.output_path,
        n_pieces=args.n_pieces
    )