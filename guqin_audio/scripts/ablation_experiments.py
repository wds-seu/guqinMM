import os
import torch
from torch.utils.data import DataLoader
from model.semantic_token_generator import SemanticTokenGenerator
from model.acoustic_token_generator import AcousticTokenGenerator
from utils.trainer import Trainer
from utils.dataset_merged import MergedGuqinDataset

def get_device():
    """è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼ˆCUDAä¼˜å…ˆï¼ŒMPSå…¶æ¬¡ï¼Œå¦åˆ™CPUï¼‰"""
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        return torch.device("mps")
    else:
        return torch.device("cpu")

def build_models(config):
    """æ ¹æ®configæ„å»ºæ¨¡å‹"""
    semantic_model = SemanticTokenGenerator(
        vocab_size=config["semantic_vocab_size"],
        embed_dim=config["embedding_dim"],
        max_seq_len=config["max_seq_len"]
    ).to(config["device"])

    acoustic_model = AcousticTokenGenerator(
        vocab_size=config["acoustic_vocab_size"],
        embed_dim=config["embedding_dim"],
        max_seq_len=config["max_seq_len"]
    ).to(config["device"])

    return semantic_model, acoustic_model

def build_dataloader(config, train=True):
    """åŠ è½½è®­ç»ƒ/éªŒè¯æ•°æ®é›†"""
    if train:
        dataset = MergedGuqinDataset([
            "data/processed/train_data.pt",
            "data/processed_75h/75h_dataset.pt"
        ])
    else:
        dataset = MergedGuqinDataset([
            "data/processed/val_data.pt"
        ])

    loader = DataLoader(dataset, batch_size=config["batch_size"], shuffle=train, drop_last=train)
    return loader

def run_experiment(exp_name, semantic_vocab_size, acoustic_vocab_size):
    """å•ç»„å®éªŒæ‰§è¡Œæµç¨‹"""
    device = get_device()
    config = {
        "device": device,
        "batch_size": 8,
        "learning_rate": 1e-4,
        "num_epochs": 20,
        "max_seq_len": 1024,
        "semantic_vocab_size": semantic_vocab_size,
        "acoustic_vocab_size": acoustic_vocab_size,
        "score_vocab_size": 5000,  # æ ¹æ®é¡¹ç›®è®¾å®š
        "style_vocab_size": 4,
        "embedding_dim": 256,
        "checkpoint_dir": f"checkpoints/{exp_name}",
        "log_dir": f"logs/{exp_name}"
    }

    os.makedirs(config["checkpoint_dir"], exist_ok=True)
    os.makedirs(config["log_dir"], exist_ok=True)

    print(f"\nğŸš€ Starting experiment: {exp_name} on device {device}")

    semantic_model, acoustic_model = build_models(config)
    train_loader = build_dataloader(config, train=True)
    val_loader = build_dataloader(config, train=False)

    # åˆå§‹åŒ–Trainer
    trainer = Trainer(semantic_model, acoustic_model, train_loader, val_loader, config)

    # å¼€å§‹è®­ç»ƒå¹¶ä¿å­˜æ¯è½®lossè®°å½•
    train_losses, val_losses = trainer.train()

    # ä¿å­˜lossæ—¥å¿—
    torch.save({
        "train_losses": train_losses,
        "val_losses": val_losses
    }, os.path.join(config["log_dir"], "loss_log.pt"))

    print(f"âœ… Experiment {exp_name} finished.\n")

if __name__ == "__main__":
    # å®šä¹‰å¯¹æ¯”å®éªŒåˆ—è¡¨ï¼ˆå¯ä»¥éšæ„å¢å‡ï¼‰
    experiments = [
        # baseline
        {"exp_name": "baseline_encodec_w2vbert", "semantic_vocab_size": 4096, "acoustic_vocab_size": 2048},

        # å£°å­¦tokenizeræ›¿æ¢
        {"exp_name": "soundstream_w2vbert", "semantic_vocab_size": 4096, "acoustic_vocab_size": 2048},

        # è¯­ä¹‰tokenizeræ›¿æ¢
        {"exp_name": "encodec_hubert", "semantic_vocab_size": 4096, "acoustic_vocab_size": 2048},

        # åŒæ›¿æ¢
        {"exp_name": "specvqvae_cpc", "semantic_vocab_size": 2048, "acoustic_vocab_size": 2048},

        # å°è¯è¡¨å‹ç¼©
        {"exp_name": "small_vocab_1024", "semantic_vocab_size": 1024, "acoustic_vocab_size": 1024},
    ]

    # æ‰§è¡Œæ‰€æœ‰å®éªŒ
    for exp in experiments:
        run_experiment(**exp)
