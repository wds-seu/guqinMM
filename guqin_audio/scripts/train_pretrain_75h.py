import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from utils.dataset_75h import AcousticTokenDataset75h
from model.model_acoustic_decoder import AcousticTransformerDecoder
from tqdm import tqdm
import argparse

def train(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Using device: {device}")

    # æµæ´¾æ ‡ç­¾æ˜ å°„
    style_mapping = {
        "exam": 0,
        "master": 1,
        "silkqin": 2,
        "other": 3
    }

    # åŠ è½½æ•°æ®é›†
    dataset = AcousticTokenDataset75h(args.acoustic_tokens_dir, style_mapping)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)

    # åˆ›å»ºæ¨¡å‹
    model = AcousticTransformerDecoder(
        vocab_size=args.vocab_size,
        style_emb_dim=args.style_emb_dim,
        d_model=args.d_model,
        nhead=args.nhead,
        num_layers=args.num_layers,
        dropout=0.1
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    criterion = nn.CrossEntropyLoss()

    model.train()
    os.makedirs(args.save_dir, exist_ok=True)

    for epoch in range(args.num_epochs):
        total_loss = 0.0

        for batch in tqdm(dataloader, desc=f"Epoch {epoch+1}/{args.num_epochs}"):
            acoustic_tokens = batch["acoustic_tokens"].to(device)  # (B, T)
            style_labels = batch["style_label"].to(device)          # (B,)

            optimizer.zero_grad()

            # è®­ç»ƒæ—¶è¾“å…¥ acoustic_tokens[:, :-1]ï¼Œç›®æ ‡æ˜¯ acoustic_tokens[:, 1:]
            input_tokens = acoustic_tokens[:, :-1]
            target_tokens = acoustic_tokens[:, 1:]

            output_logits = model(input_tokens, style_labels)  # (B, T-1, vocab_size)
            output_logits = output_logits.reshape(-1, args.vocab_size)
            target_tokens = target_tokens.reshape(-1)

            loss = criterion(output_logits, target_tokens)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f"âœ… Epoch {epoch+1}, Average Loss: {avg_loss:.4f}")

        # ä¿å­˜æ¨¡å‹
        torch.save(model.state_dict(), os.path.join(args.save_dir, f"pretrain_epoch{epoch+1}.pt"))

def collate_fn(batch):
    """
    è‡ªå®šä¹‰collate_fnï¼Œè¿›è¡ŒåŠ¨æ€padding
    """
    acoustic_tokens = [sample["acoustic_tokens"] for sample in batch]
    style_labels = torch.tensor([sample["style_label"] for sample in batch], dtype=torch.long)

    # åŠ¨æ€paddingåˆ°åŒä¸€é•¿åº¦
    max_len = max([len(x) for x in acoustic_tokens])
    padded_tokens = torch.zeros((len(batch), max_len), dtype=torch.long)

    for i, tokens in enumerate(acoustic_tokens):
        padded_tokens[i, :len(tokens)] = tokens

    return {
        "acoustic_tokens": padded_tokens,
        "style_label": style_labels
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument('--acoustic_tokens_dir', type=str, required=True, help="75hå£°å­¦tokensç›®å½•")
    parser.add_argument('--save_dir', type=str, default="checkpoints_pretrain_75h", help="ä¿å­˜æ¨¡å‹çš„è·¯å¾„")
    parser.add_argument('--vocab_size', type=int, default=1024, help="å£°å­¦tokenè¯è¡¨å¤§å°")
    parser.add_argument('--style_emb_dim', type=int, default=256, help="æµæ´¾åµŒå…¥å‘é‡ç»´åº¦")
    parser.add_argument('--d_model', type=int, default=512, help="Transformeréšè—å±‚ç»´åº¦")
    parser.add_argument('--nhead', type=int, default=8, help="å¤šå¤´æ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument('--num_layers', type=int, default=6, help="Transformerå±‚æ•°")
    parser.add_argument('--batch_size', type=int, default=16, help="batchå¤§å°")
    parser.add_argument('--lr', type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument('--num_epochs', type=int, default=20, help="è®­ç»ƒè½®æ•°")

    args = parser.parse_args()
    train(args)
