# inference.py
import os
import torch
from model.transformer_model import StreamingTransformerModel

def load_model(checkpoint_path, device):
    model = StreamingTransformerModel(
        vocab_size_score=128,
        vocab_size_acoustic=1024,
        style_emb_dim=256,
        d_model=256,
        nhead=8,
        num_layers=6
    )
    ckpt = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(ckpt["model_state_dict"])
    model = model.to(device)
    model.eval()
    return model

@torch.no_grad()
def inference(model, score_tokens, style_labels, max_length=2048, device="cuda"):
    """
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        score_tokens: (B, T_score)  è°±æ–‡tokenï¼ŒLongTensor
        style_labels: (B)           é£æ ¼æ ‡ç­¾ï¼ŒLongTensor
        max_length: ç”Ÿæˆçš„å£°å­¦tokenæœ€å¤§é•¿åº¦
    Returns:
        generated_acoustic_tokens: (B, max_length)
    """
    B = score_tokens.size(0)
    generated_tokens = []
    states = None
    offset = 0

    prev_tokens = torch.zeros((B, 1), dtype=torch.long, device=device)  # start tokenï¼ˆä½ å¯ä»¥æ¢æˆåˆ«çš„ç‰¹æ®Štokenï¼‰
    
    for step in range(max_length):
        logits, states, offset = model(score_tokens, style_labels, prev_tokens, states, offset)
        logits = logits[:, -1, :]  # å–æœ€åä¸€ä¸ªstepçš„é¢„æµ‹ (B, vocab_size)

        next_token = torch.argmax(logits, dim=-1, keepdim=True)  # (B, 1)

        generated_tokens.append(next_token)

        prev_tokens = next_token  # ä¸‹ä¸€æ­¥ç»§ç»­å–‚

    generated_tokens = torch.cat(generated_tokens, dim=1)  # (B, max_length)
    return generated_tokens

def main():
    # ==== é…ç½® ====
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    checkpoint_path = "exp/finetune/checkpoint_epoch50.pt"  # ä½ çš„æœ€ç»ˆå¾®è°ƒæ¨¡å‹
    output_dir = "exp/inference_results"
    os.makedirs(output_dir, exist_ok=True)

    # ==== å‡†å¤‡è¾“å…¥ ====
    # TODO: è¿™é‡Œä½ è¦æ¢æˆè‡ªå·±çš„è°±æ–‡tokenså’Œstyle_labelsï¼Œæ¯”å¦‚5é¦–æµ‹è¯•æ›²å­
    # ä¸¾ä¸ªä¾‹å­ï¼š
    score_tokens = torch.randint(0, 128, (2, 100)).to(device)  # å‡è®¾2é¦–æ›²å­ï¼Œæ¯é¦–100ä¸ªè°±æ–‡token
    style_labels = torch.tensor([0, 2], dtype=torch.long).to(device)  # å‡è®¾ä¸€é¦– examï¼Œä¸€é¦– silkqin

    # ==== åŠ è½½æ¨¡å‹ ====
    model = load_model(checkpoint_path, device)

    # ==== å¼€å§‹æ¨ç† ====
    print("ğŸš€ Start inference...")
    generated_acoustic_tokens = inference(model, score_tokens, style_labels, max_length=2048, device=device)
    print("âœ… Inference done!")

    # ==== ä¿å­˜è¾“å‡º ====
    save_path = os.path.join(output_dir, "generated_tokens.pt")
    torch.save({
        "score_tokens": score_tokens.cpu(),
        "style_labels": style_labels.cpu(),
        "generated_acoustic_tokens": generated_acoustic_tokens.cpu()
    }, save_path)

    print(f"ğŸµ Generated acoustic tokens saved at {save_path}")

if __name__ == "__main__":
    main()
